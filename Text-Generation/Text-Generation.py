# -*- coding: utf-8 -*-
"""Text-Generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MHZIcym_SVO2CMzkfiGLUDbHzdfnibGr
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset

import matplotlib.pyplot as plt
import numpy as np
import random
import pandas as pd
import re
import regex
from collections import Counter

import matplotlib.pyplot as plt

from wordcloud import WordCloud

"""### Data observation

> Retrieve the song data from the file provided
"""

songs_df = pd.read_csv('Songs.csv', header = 0)
songs_df.head(5)

"""> Each entry ends with something like '1EmbedShare URLCopyEmbedCopy', so we removed those appearances from the lyrics."""

songs_df['Lyrics'] = songs_df['Lyrics'].str.replace(r'\d+EmbedShare URLCopyEmbedCopy$', "", regex=True)

"""> Show all unique artists in the dataset"""

for artist in songs_df['Artist'].unique():
    print(f'{artist}: {songs_df[songs_df['Artist'] == artist].shape[0]} Songs')

"""> The dataset's size."""

print(f'The dataset contains {len(songs_df)} entries for songs.')

"""> The dataset contains 3 duplicates so we will need to take care of that."""

print(f'The dataset contains {len(songs_df['Title'].unique())} unique songs.')

"""> Cleanup of duplicates"""

songs_df = songs_df.drop_duplicates(subset='Title')

for artist in songs_df['Artist'].unique():
    print(f'{artist}: {songs_df[songs_df['Artist'] == artist].shape[0]} Songs')

"""> Average song length (characters & words)"""

lengths = songs_df['Lyrics'].apply(lambda x: len(x.split()))
avg_words = lengths.sum() / songs_df['Lyrics'].nunique()

char_lengths = songs_df['Lyrics'].apply(lambda x: len(x))
avg_chars = char_lengths.sum() / songs_df['Lyrics'].nunique()

print(f'The average number of words in a song is {avg_words:.2f}')
print(f'The average number of characters in a song is {avg_chars:.2f}')

"""> Word Cloud

> First we checked for abnormal characters we wanted to remove
"""

# Combine all lyrics into one large text
all_lyrics = " ".join(songs_df["Lyrics"].dropna())

# Clean the text (remove punctuation, convert to lowercase)
all_lyrics = re.sub(r"[^\w\s]", "", all_lyrics.lower())

print(set(all_lyrics))

"""> Tokenize and remove words that contain characters that are not latin or digits"""

# Tokenization and cleaning
words = all_lyrics.split()

# \p{L} matches any letter in any language
# \p{N} matches any numeric character
words = [word for word in words if regex.fullmatch(r"[\p{Latin}\p{N}]+", word)]

"""> Set up the WordCloud"""

# Retrieve word frequencies
word_counts = Counter(words)

# 3 most common words
print("The 3 most common words are:")
for word, count in word_counts.most_common(3):
    print(f"{word}: {count} appearances")

# Definition of wordCloud
wordcloud = WordCloud(width=800, height=400, background_color="white", colormap='plasma', max_words=50).generate_from_frequencies(word_counts)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

"""### Preprocessing

> Build a corpus and encode it for training
"""

lyrics = songs_df["Lyrics"].dropna().str.lower()

# Remove words with unwanted characters
tokenized_lyrics = [
    [word if word != '\n' else '<newline>' for word in regex.findall(r"[\p{Latin}\p{N}]+|\n", lyric)]
    for lyric in lyrics
]

# Build vocabulary
word_counts = Counter(word for song in tokenized_lyrics for word in song)
word_to_index = {word: i + 1 for i, (word, _) in enumerate(word_counts.items())}
index_to_word = {i: word for word, i in word_to_index.items()}

# Vocabulary size
vocab_size = len(word_to_index) + 1  # Adding 1 for padding if needed

print(f"Vocabulary Size: {vocab_size}")

"""> We printed the filtered characters, only words with these characters are considered"""

all_chars = "".join(word for lyric in tokenized_lyrics for word in lyric)
print(sorted(set(all_chars)))

# Convert lyrics to numerical sequences, instead of words it is now numbers (indices)
encoded_sequences = [[word_to_index[word] for word in song if word in word_to_index] for song in tokenized_lyrics]

# Set sequence length (choose a reasonable length)
# This is the number of words that the model will use to predict the next word
SEQ_LENGTH = 20

# Create input-target pairs
input_sequences = []
targets = []

# Construct all sequences and their targets
for song in encoded_sequences:
    for i in range(1, len(song)):
        n_gram_sequence = song[:i+1]                        # Create sequence up to current word
        if len(n_gram_sequence) >= 2:                       # Ensure sequence has at least one input and one target
            input_sequences.append(n_gram_sequence[:-1])    # All but last word (input)
            targets.append(n_gram_sequence[-1])             # Last word (target)

# Pad sequences to the same length (max length)
# we use 0 for padding since the indices start from 1
max_seq_length = SEQ_LENGTH
padded_sequences = [([0] * (max_seq_length - len(seq)) + seq)[-max_seq_length:] for seq in input_sequences]

# Convert to tensors
X = torch.tensor(padded_sequences, dtype=torch.long)
y = torch.tensor(targets, dtype=torch.long)

class SongLyricsDataset(Dataset):
    def __init__(self, X, y):
        self.X = X
        self.y = y

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# Create DataLoader
dataset = SongLyricsDataset(X, y)
data_loader = DataLoader(dataset, batch_size=64, shuffle=True)

print(f"Dataset Size: {len(dataset)}")

class LyricsLSTM(nn.Module):
    def __init__(self, vocab_size, embedding_dim=256, hidden_dim=256, num_layers=4):
        super(LyricsLSTM, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, vocab_size)

    def forward(self, x):
        x = self.embedding(x)
        lstm_out, _ = self.lstm(x)
        out = self.fc(lstm_out[:, -1, :])  # Get output from last LSTM step
        return out

# Model setup
model = LyricsLSTM(vocab_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# GPU utilization
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

"""> Train for 100 Epochs as requested"""

EPOCHS = 100

train_losses = []

for epoch in range(EPOCHS):
    model.train()
    total_loss = 0

    for batch_X, batch_y in data_loader:
        batch_X, batch_y = batch_X.to(device), batch_y.to(device)

        optimizer.zero_grad()
        predictions = model(batch_X)
        loss = criterion(predictions, batch_y)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(data_loader)
    train_losses.append(avg_loss)  # Save the average loss for this epoch
    if epoch%5 == 0:
        print(f"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss/len(data_loader):.4f}")

"""#### Optional save/load

> Save the model after training (Optional)
"""

torch.save(model.state_dict(), 'lyrics_lstm_weights.pth')

"""> Load the model (Optional)"""

model = LyricsLSTM(vocab_size)
model.load_state_dict(torch.load('lyrics_lstm_weights.pth'))
model.eval()

"""#### Visualization

> Plot the training loss
"""

# Plotting the training loss after training completes
plt.figure(figsize=(8, 5))
plt.plot(range(1, EPOCHS + 1), train_losses, marker='o', label='Training Loss')
plt.title('Training Loss over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

"""#### Lyrics Generator"""

def LyricsGenerator(
        starting_string, model, word_to_index, index_to_word,
        max_words=avg_words,
        strategy="",
        k= 10, p = 0.9, temperature = 1.0):

    model.eval()
    max_words = int(max_words)

    if not starting_string.strip():
        starting_word = random.choice(list(word_to_index.keys()))
        words = [starting_word]
    else:
        words = starting_string.lower().split()

    for _ in range(max_words):
        # Convert words to indices
        encoded_input = [word_to_index.get(word, 0) for word in words][-SEQ_LENGTH:]
        input_tensor = torch.tensor([encoded_input], dtype=torch.long).to(device)

        # Predict next word
        with torch.no_grad():
            output = model(input_tensor)
            output = output.squeeze(0)

            logits = output / temperature
            probabilities = torch.softmax(logits, dim=-1).cpu().numpy()

            if strategy == "top-k":
                top_k_indices = probabilities.argsort()[-k:][::-1]
                top_k_probs = probabilities[top_k_indices]
                top_k_probs = top_k_probs / top_k_probs.sum()  # Normalization to make a distribution
                predicted_index = np.random.choice(top_k_indices, p=top_k_probs)
            elif strategy == "top-p":
                sorted_indices = np.argsort(probabilities)[::-1]
                cumulative_probs = np.cumsum(probabilities[sorted_indices])
                top_p_indices = sorted_indices[cumulative_probs <= p]

                if len(top_p_indices) == 0:  # If no words remain, default to the word with highest probability
                    top_p_indices = [sorted_indices[0]]

                top_p_probs = probabilities[top_p_indices]
                top_p_probs = top_p_probs / top_p_probs.sum()  # Normalize to make it a valid probability distribution
                predicted_index = np.random.choice(top_p_indices, p=top_p_probs)
            elif strategy == "greedy":
                predicted_index = np.argmax(probabilities)
            else:
                predicted_index = np.random.choice(len(probabilities), p=probabilities)

        # Convert index to word
        next_word = index_to_word.get(predicted_index, "<UNK>")
        words.append(next_word)

    return " ".join(words)

"""#### Using top-k strategy"""

seed = "it's not a silly little moment"
generated_text = LyricsGenerator(seed, model, word_to_index, index_to_word, strategy="top-k", k=10)
generated_text = generated_text.replace('<newline>', '\n')
print("\nGenerated Lyrics:\n", generated_text)

seed = "There is a house in"
generated_text = LyricsGenerator(seed, model, word_to_index, index_to_word, strategy="top-k", k=10)
generated_text = generated_text.replace('<newline>', '\n')
print("\nGenerated Lyrics:\n", generated_text)

seed = ""
generated_text = LyricsGenerator(seed, model, word_to_index, index_to_word, strategy="top-k", k=15)
generated_text = generated_text.replace('<newline>', '\n')
print("\nGenerated Lyrics:\n", generated_text)

"""#### Using top-p strategy"""

seed = "it's not a silly little moment"
generated_text = LyricsGenerator(seed, model, word_to_index, index_to_word, strategy="top-p", p=0.9)
generated_text = generated_text.replace('<newline>', '\n')
print("\nGenerated Lyrics:\n", generated_text)

seed = "There is a house in"
generated_text = LyricsGenerator(seed, model, word_to_index, index_to_word, strategy="top-p", p=0.9)
generated_text = generated_text.replace('<newline>', '\n')
print("\nGenerated Lyrics:\n", generated_text)

seed = ""
generated_text = LyricsGenerator(seed, model, word_to_index, index_to_word, strategy="top-p", p=0.86)
generated_text = generated_text.replace('<newline>', '\n')
print("\nGenerated Lyrics:\n", generated_text)

"""#### Using Greedy"""

seed = "it's not a silly little moment"
generated_text = LyricsGenerator(seed, model, word_to_index, index_to_word, strategy="greedy")
generated_text = generated_text.replace('<newline>', '\n')
print("\nGenerated Lyrics:\n", generated_text)

seed = "there is a house in"
generated_text = LyricsGenerator(seed, model, word_to_index, index_to_word, strategy="greedy")
generated_text = generated_text.replace('<newline>', '\n')
print("\nGenerated Lyrics:\n", generated_text)

seed = ""
generated_text = LyricsGenerator(seed, model, word_to_index, index_to_word, strategy="greedy")
generated_text = generated_text.replace('<newline>', '\n')
print("\nGenerated Lyrics:\n", generated_text)

"""#### Final Note

<font color='aqua'>The strategies produced results that are somewhat indistinguishable regarding their quality, in general we would tend to use the top-p approach because we would like to encourage the model to be more 'creative'. It is also important to note that the greedy algorithm will produce the same output for a given starting string whereas top-p and top-k will produce a different output each time.</font>
"""